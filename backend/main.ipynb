{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prash\\OneDrive\\Desktop\\MajorProject-Rana\\Project\\backend\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, BertModel, BertTokenizer\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a custom dataset (replace with real authorship data)\n",
    "# This is a placeholder; you will need to replace this with your own dataset (texts, authorship labels)\n",
    "dataset = load_dataset('ag_news', split='train[:5%]')  # Small sample for testing\n",
    "texts = dataset['text']\n",
    "labels = dataset['label']  # You should replace this with actual authorship labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GPT-2 (or BERT for a different approach)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')  # Change to 'bert-base-uncased' for BERT\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')  # Change to BertModel.from_pretrained for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_embeddings(texts, model, tokenizer):\n",
    "    embeddings = []\n",
    "\n",
    "    # Ensure that the GPT-2 tokenizer has a pad_token\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token for GPT-2\n",
    "\n",
    "    for text in texts:\n",
    "        # Tokenize text with padding, truncation, and attention masks\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        # Make sure model is set to output hidden states\n",
    "        model.config.output_hidden_states = True\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        # The hidden states are returned in a tuple of tensors\n",
    "        # outputs.hidden_states[-1] is the last layer hidden states (shape: [batch_size, seq_len, hidden_size])\n",
    "        last_hidden_state = outputs.hidden_states[-1]  # Take the last layer's hidden states\n",
    "        embeddings.append(last_hidden_state.mean(dim=1).squeeze().numpy())  # Mean pooling over the sequence length\n",
    "\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Extract LLM embeddings for the texts\n",
    "llm_embeddings = get_llm_embeddings(texts, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_graph_from_embeddings(embeddings, threshold=0.8):\n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "    # Create edges for similarity above threshold\n",
    "    edge_index = np.array(np.nonzero(similarity_matrix > threshold))\n",
    "\n",
    "    return torch.tensor(edge_index, dtype=torch.long)\n",
    "\n",
    "# Construct the graph based on LLM embeddings\n",
    "edge_index = construct_graph_from_embeddings(llm_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuthorshipGNN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels, num_classes):\n",
    "        super(AuthorshipGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert LLM embeddings to a PyTorch tensor\n",
    "x = torch.tensor(llm_embeddings, dtype=torch.float)\n",
    "\n",
    "# Prepare graph data in PyTorch Geometric format\n",
    "data = Data(x=x, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 10.0325\n",
      "Epoch 2, Loss: 7.7568\n",
      "Epoch 3, Loss: 11.2341\n",
      "Epoch 4, Loss: 16.9262\n",
      "Epoch 5, Loss: 13.7347\n",
      "Epoch 6, Loss: 7.1787\n",
      "Epoch 7, Loss: 8.7896\n",
      "Epoch 8, Loss: 9.5433\n",
      "Epoch 9, Loss: 7.6898\n",
      "Epoch 10, Loss: 5.6032\n"
     ]
    }
   ],
   "source": [
    "# Initialize the GNN model\n",
    "gnn_model = AuthorshipGNN(num_features=llm_embeddings.shape[1], hidden_channels=64, num_classes=4)  # For 4 classes in AG News\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(gnn_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train function\n",
    "def train(model, data, optimizer, labels):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = F.nll_loss(out, torch.tensor(labels[:len(out)], dtype=torch.long))  # Match labels to prediction size\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Adjust number of epochs for your dataset\n",
    "    loss = train(gnn_model, data, optimizer, labels)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.2578\n"
     ]
    }
   ],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, data, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "    accuracy = (pred == torch.tensor(labels[:len(pred)], dtype=torch.long)).sum().item() / len(pred)\n",
    "    return accuracy\n",
    "\n",
    "accuracy = evaluate(gnn_model, data, labels)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the graph (Optional)\n",
    "def visualize_graph(similarity_matrix, threshold=0.8):\n",
    "    G = nx.Graph()\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        G.add_node(i, label=str(i))\n",
    "    for i, j in zip(*np.where(similarity_matrix > threshold)):\n",
    "        G.add_edge(i, j)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    nx.draw(G, with_labels=True, node_color='skyblue', font_size=10)\n",
    "    plt.show()\n",
    "    \n",
    "visualize_graph(cosine_similarity(llm_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "# Define FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define input format\n",
    "class AuthorshipRequest(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/verify_authorship/\")\n",
    "async def verify_authorship(request: AuthorshipRequest):\n",
    "    # Process the text and get LLM embeddings\n",
    "    embedding = get_llm_embeddings([request.text], model, tokenizer)\n",
    "\n",
    "    # Make prediction using the trained GNN model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = gnn_model(torch.tensor(embedding, dtype=torch.float), data.edge_index)\n",
    "    pred = out.argmax(dim=1).item()\n",
    "\n",
    "    return {\"predicted_label\": pred}\n",
    "\n",
    "# Run the FastAPI app (in production, use an ASGI server like Uvicorn)\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Testing the API\n",
    "import requests\n",
    "\n",
    "response = requests.post(\"http://127.0.0.1:8000/verify_authorship/\", json={\"text\": \"Sample text to verify authorship.\"})\n",
    "print(response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
